---
title: "Game of Thrones - Sentiment analysis"
author: "Ellen Juul Randb√∏ll"
date: "2025-03-21"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("here")
#install.packages("scales")
library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
library(scales)
```
### Get the GOT-text:
```{r get-document}

GOT_path <- "C:\\Users\\ellen\\OneDrive - Aarhus universitet\\Digitale arkiver og metoder\\W12\\SentimentAnalysis-main\\data\\got.pdf"
GOT_text <- pdf_text(GOT_path)
```

# Example: How to get text from a single page (e.g. Page 15)
```{r single-page}
GOT_p15 <- GOT_text[15]
GOT_p15
```

### Some wrangling:

- I split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- I unnest into regular columns using `tidyr::unnest()`
- I remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
GOT_df <- data.frame(GOT_text) %>% 
  mutate(text_full = str_split(GOT_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

I use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. I am interested in *words*, so that's the token I'll use:

```{r tokenize}
GOT_tokens <- GOT_df %>% 
  unnest_tokens(word, text_full)
GOT_tokens
```


I count the words:
```{r count-words}
GOT_wc <- GOT_tokens %>% 
  count(word) %>% 
  arrange(-n)
GOT_wc
```

### Remove stop words:

I will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
GOT_stop <- GOT_tokens %>% 
  anti_join(stop_words) %>% 
  select(-GOT_text)
```

Now I check the counts again: 
```{r count-words2}
GOT_swc <- GOT_stop %>% 
  count(word) %>% 
  arrange(-n)
```


Now I want to get rid of all the numbers (non-text):
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

GOT_no_numeric <- GOT_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of words in Game of Thrones (non-numeric)


```{r wordcloud-prep}
# There are 11209 unique words 
length(unique(GOT_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
GOT_top100 <- GOT_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
GOT_cloud <- ggplot(data = GOT_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

GOT_cloud
```


I now customize it a bit:
```{r wordcloud-pro}
ggplot(data = GOT_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

### Sentiment analysis
"afinn": Words ranked from -5 (very negative) to +5 (very positive)
bing: binary, "positive" or "negative"
Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

```{r get_sentiments}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "nrc")
```

### Sentiment analysis with afinn: 

First, I bind words in `GOT_stop` to `afinn` lexicon:
```{r bind-afinn}
GOT_afinn <- GOT_stop %>% 
  inner_join(get_sentiments("afinn"))
```
I then find some counts (by sentiment ranking):
```{r count-afinn}
GOT_afinn_hist <- GOT_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = GOT_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()+
  labs(x= "Sentiment value",y="Frequency")
```

I investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
GOT_afinn2 <- GOT_afinn %>% 
  filter(value == 2)

# Check the unique 2-score words:
unique(GOT_afinn2$word)

# Count & plot them
GOT_afinn2_n <- GOT_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))

GOT_afinn2_n2 <- head(GOT_afinn2_n, n=40)

ggplot(data = GOT_afinn2_n2, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()+
  labs(x="Word", y="Frequency")
```
I can summarize sentiment for Game of Thrones: 
```{r summarize-afinn}
GOT_summary <- GOT_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )

GOT_summary
```

### NRC lexicon for sentiment analysis
I can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, I'll use inner_join() to combine the GOT non-stopword text with the nrc lexicon: 

```{r bind-bing}
GOT_nrc <- GOT_stop %>% 
  inner_join(get_sentiments("nrc"))
```

I now check which words are excluded using `anti_join()`:

```{r check-exclusions}
GOT_exclude <- GOT_stop %>% 
  anti_join(get_sentiments("nrc"))

View(GOT_exclude)

# Count to find the most excluded:
GOT_exclude_n <- GOT_exclude %>% 
  count(word, sort = TRUE)

head(GOT_exclude_n)
```

I now find some counts: 
```{r count-bing}
GOT_nrc_n <- GOT_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = GOT_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()+
  scale_y_continuous(labels = label_number())+
  labs(x="Sentiment", y="Frequency")
```

I can also count by sentiment *and* word, then facet:
```{r count-nrc}
GOT_nrc_n5 <- GOT_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

GOT_nrc_gg <- ggplot(data = GOT_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "Frequency")

# Show it
GOT_nrc_gg

# Save it
ggsave(plot = GOT_nrc_gg, 
       here("W12/SentimentAnalysis-main/figures","GOT_nrc_sentiment.png"), 
       height = 15, 
       width = 5)

```
Some word are placed in several categories, fx the word "lord".
The word "lord" is placed in four different categories: negative, positive, disgust, trust 
```{r nrc-lord}

lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

lord

```